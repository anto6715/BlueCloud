{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Ocean regimes indicator__: Model development notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Gaussian Mixtures Model trained with a dataset of time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "ds : initial dataset (lat, lon, week)\n",
    "\n",
    "X : staked dataset (sampling, week, week_reduced)\n",
    "\n",
    "ds_labels: unstacked final dataset (lat, lon, week)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import pyxpcm\n",
    "#from pyxpcm.models import pcm\n",
    "\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.colors as mcolors\n",
    "#import matplotlib as mpl\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.set_context(\"paper\")\n",
    "#with_seaborn = True\n",
    "\n",
    "#from PIL import Image\n",
    "\n",
    "#import importlib\n",
    "\n",
    "import Plotter_OR\n",
    "from Plotter_OR import Plotter_OR \n",
    "\n",
    "from BIC_calculation_OR import *\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "K=7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Choose training dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One year of Ocean Color data in the Mediterranean Sea:  OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082 product\n",
    "\n",
    ".........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMEMS_user = 'agarcia6'\n",
    "CMEMS_password = '1802910672Aa#'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geographical extent\n",
    "geo_extent = [-5, 42, 30, 46] # [min lon, max lon, min lat, max lat]\n",
    "# time extent\n",
    "time_extent = [\"2019-01-01\", \"2018-12-31\"] # [\"min date\", \"max date\"]\n",
    "# variable to be predict\n",
    "var_name = 'CHL' # name in dataset\n",
    "# file name\n",
    "file_name = 'oceancolour_glo_chl_l4_rep_observations_009_082_2019.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load training dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m motuclient -u agarcia6 -p 1802910672Aa# -m \"http://my.cmems-du.eu/motu-web/Motu\" -s OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082-TDS -d dataset-oc-glo-bio-multi-l4-chl_interpolated_4km_daily-rep -x -5 -X 42 -y 30 -Y 46 -t \"2019-01-01\" -T \"2018-12-31\" -z 0.0 -Z 2500.0 -v CHL -o datasets -f oceancolour_glo_chl_l4_rep_observations_009_082_2019.nc\n"
     ]
    }
   ],
   "source": [
    "bashCommand = 'python -m motuclient -u ' + CMEMS_user + ' -p ' + CMEMS_password + ' -m \"http://my.cmems-du.eu/motu-web/Motu\" \\\n",
    "-s OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009_082-TDS -d dataset-oc-glo-bio-multi-l4-chl_interpolated_4km_daily-rep \\\n",
    "-x ' + str(geo_extent[0]) + ' -X ' + str(geo_extent[1]) + ' -y ' + str(geo_extent[2]) + ' -Y ' + str(geo_extent[3]) + \\\n",
    "' -t \"' + time_extent[0] + '\" -T \"' + time_extent[1] + '\" -z 0.0 -Z 2500.0 \\\n",
    "-v ' + var_name + ' -o datasets -f ' + file_name\n",
    "print(bashCommand)\n",
    "sp = subprocess.call(bashCommand, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = 'datasets/dataset-oc-glo-bio-multi-l4-chl_interpolated_4km_daily-rep_1611225376956.nc' #in datarmor\n",
    "file_path = 'datasets/dataset-oc-glo-bio-multi-l4-chl_interpolated_4km_daily-rep_1610026811620.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open dataset\n",
    "\n",
    "........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (lat: 385, lon: 1128, time: 1)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2019-01-01\n",
      "  * lat      (lat) float32 46.020832 45.979164 45.9375 ... 30.062498 30.020832\n",
      "  * lon      (lon) float32 -4.9791613 -4.9374948 ... 41.937508 41.97917\n",
      "Data variables:\n",
      "    CHL      (time, lat, lon) float32 ...\n",
      "Attributes:\n",
      "    comment:                      average\n",
      "    distribution_statement:       See CMEMS Data License\n",
      "    site_name:                    GLO\n",
      "    parameter_code:               CHL\n",
      "    creation_time:                16:39:54 UTC\n",
      "    creation_date:                2020-11-05 UTC\n",
      "    nb_grid_bins:                 37324800\n",
      "    easternmost_longitude:        180.0\n",
      "    references:                   http://www.globcolour.info GlobColour has b...\n",
      "    registration:                 5\n",
      "    stop_date:                    2020-07-01 UTC\n",
      "    lon_step:                     0.041666668\n",
      "    cmems_product_id:             OCEANCOLOUR_GLO_CHL_L4_REP_OBSERVATIONS_009...\n",
      "    naming_authority:             CMEMS\n",
      "    northernmost_valid_latitude:  86.666664\n",
      "    grid_mapping:                 Equirectangular\n",
      "    title:                        dataset-oc-glo-bio-multi-l4-chl_interpolate...\n",
      "    period_end_day:               20200630\n",
      "    duration_time:                PT151043S\n",
      "    DPM_reference:                GC-UD-ACRI-PUG\n",
      "    product_name:                 20200630_d-ACRI-L4-CHL-MULTI_4KM-GLO-REP\n",
      "    netcdf_version_id:            4.3.3.1 of Jul  8 2016 18:15:50 $\n",
      "    platform:                     Aqua,Suomi-NPP,Sentinel-3a,JPSS-1 (NOAA-20)...\n",
      "    northernmost_latitude:        90.0\n",
      "    IODD_reference:               GC-UD-ACRI-PUG\n",
      "    stop_time:                    08:47:59 UTC\n",
      "    file_quality_index:           0\n",
      "    start_date:                   2020-06-29 UTC\n",
      "    product_level:                4\n",
      "    period_start_day:             20200630\n",
      "    software_name:                globcolour_l3_reproject\n",
      "    citation:                     The Licensees will ensure that original CME...\n",
      "    software_version:             2020.0\n",
      "    start_time:                   14:50:37 UTC\n",
      "    easternmost_valid_longitude:  180.00002\n",
      "    nb_bins:                      37324800\n",
      "    southernmost_valid_latitude:  -54.458336\n",
      "    nb_equ_bins:                  8640\n",
      "    lat_step:                     0.041666668\n",
      "    sensor_name_list:             MOD,VIR,OLA,VJ1,OLB\n",
      "    institution:                  ACRI\n",
      "    publication:                  Gohin, F., Druon, J. N., Lampert, L. (2002)...\n",
      "    grid_resolution:              4.6383123\n",
      "    sensor_name:                  MODISA,VIIRSN,OLCIa,VIIRSJ1,OLCIb\n",
      "    product_type:                 day\n",
      "    sensor:                       Moderate Resolution Imaging Spectroradiomet...\n",
      "    Conventions:                  CF-1.4\n",
      "    westernmost_valid_longitude:  -180.0\n",
      "    contact:                      servicedesk.cmems@acri-st.fr\n",
      "    westernmost_longitude:        -180.0\n",
      "    source:                       surface observation\n",
      "    earth_radius:                 6378.137\n",
      "    southernmost_latitude:        -90.0\n",
      "    pct_bins:                     100.0\n",
      "    cmems_production_unit:        OC-ACRI-NICE-FR\n",
      "    period_duration_day:          P1D\n",
      "    history:                      \n",
      "    nb_valid_bins:                18893040\n",
      "    pct_valid_bins:               50.61792695473251\n",
      "    parameter:                    Chlorophyll-a concentration\n",
      "    History:                      Translated to CF-1.0 Conventions by Netcdf-...\n",
      "    geospatial_lat_min:           30.020832061767578\n",
      "    geospatial_lat_max:           46.02083206176758\n",
      "    geospatial_lon_min:           -4.979161262512207\n",
      "    geospatial_lon_max:           41.97917175292969\n"
     ]
    }
   ],
   "source": [
    "file_path = 'datasets/' + file_name\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "ds['time'] = ds.indexes['time'].to_datetimeindex()\n",
    "\n",
    "print(ds)\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quick plot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[var_name].isel(time=11).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(int(ds[var_name].min().values), int(ds[var_name].max().values))\n",
    "ds[var_name].plot.hist(bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1) Weekly mean for each pixel__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.groupby(\"time.week\").mean()\n",
    "#ds = ds.groupby(\"time.month\").mean()\n",
    "print(ds)\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(int(X[var_name].min().values), int(X[var_name].max().values),0.5)\n",
    "#bins = np.arange(0, 0.3, 0.002)\n",
    "histo_2d = [] \n",
    "for iweek in range(52):\n",
    "#for iweek in range(12):\n",
    "    hist_values, bin_edges = np.histogram(X[var_name].isel(week=iweek).values, bins=bins)\n",
    "    #hist_values, bin_edges = np.histogram(ds[var_name].isel(month=iweek).values, bins=bins)\n",
    "    histo_2d.append(hist_values)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "#plt.pcolormesh(bins, ds.month.values, histo_2d, cmap='Reds', edgecolors='black')\n",
    "plt.pcolormesh(bins, X.week.values, histo_2d, cmap='Reds', edgecolors='black')\n",
    "cbar = plt.colorbar()\n",
    "ax.set_xlabel('Temperature (K)')\n",
    "#ax.set_xlabel('Chlorophyll-a concentration (milligram m-3)')\n",
    "ax.set_ylabel('Weeks')\n",
    "cbar.ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __2) Reduce lat lon dimensions to sampling dimension__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dims = list(X.dims)\n",
    "sampling_dims.remove('week')\n",
    "#sampling_dims.remove('time')\n",
    "sampling_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.stack({'sampling': sampling_dims})\n",
    "X = X.rename_dims({'week': 'feature'})\n",
    "X = X.rename({'week': 'feature'})\n",
    "#X = X.rename_dims({'time': 'feature'})\n",
    "#X = X.rename({'time': 'feature'})\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[var_name].plot.hist(bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3) Delate all NaN time series using mask__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chlorophyll mask__: Create mask from another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file = 'datasets/med00-ogs-bio-an-fc-d_1611233647884.nc'\n",
    "mask = xr.open_dataset(mask_file)\n",
    "mask = mask.squeeze()\n",
    "mask = mask.rename_dims({'latitude': 'lat', 'longitude':'lon'})\n",
    "mask = mask.rename({'latitude': 'lat', 'longitude':'lon'})\n",
    "mask = mask.interp_like(ds)\n",
    "mask = mask['nppv'].notnull()\n",
    "#print(ds)\n",
    "mask.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_mask = mask.stack({'sampling': sampling_dims})\n",
    "print(stacked_mask)\n",
    "#stacked_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[var_name].where(stacked_mask == True, drop=True).to_dataset()\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delate time series that are all NaN, not taken in to acount in the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[var_name].where(~X[var_name].isnull(),drop=True).to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the dataset (unravel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unstacked = X[var_name].unstack('sampling').to_dataset(name = var_name)\n",
    "X_unstacked = X_unstacked.sortby(['lat','lon'])\n",
    "print(np.shape(X_unstacked[var_name]))\n",
    "# same lat and lon values in mask and in results\n",
    "mask = stacked_mask.unstack()\n",
    "X_unstacked = X_unstacked.reindex_like(mask)\n",
    "print(np.shape(X_unstacked[var_name]))\n",
    "print(X_unstacked) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unstacked[var_name].isel(feature=11).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There is any NaN in the dataset?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.any(np.isnan(X[var_name].values))\n",
    "np.sum(np.isnan(X[var_name].values))\n",
    "\n",
    "#np.argwhere(np.isnan(X[var_name].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __4) Interpolation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessary if using mask created from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X[var_name].interpolate_na(dim = 'feature', method=\"linear\", fill_value=\"extrapolate\").to_dataset(name = var_name)\n",
    "X = X[var_name].interpolate_na(dim = 'feature', method=\"linear\", fill_value=\"extrapolate\").to_dataset(name = var_name)\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There is any NaN in the dataset?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.any(np.isnan(X[var_name].values))\n",
    "np.sum(np.isnan(X[var_name].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __5) Scaler__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose the best scaler: https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check dimensions order__\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose dataset if needed (sampling x features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.transpose(\"sampling\", \"feature\")\n",
    "np.shape(X[var_name].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply sklearn __StandardScaler__: \n",
    "Standardize features by removing the mean and scaling to unit variance\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "    z = (x - u) / s\n",
    "    \n",
    "where u is the mean of the training samples or zero if `with_mean=False`, and s is the standard deviation of the training samples or one if `with_std=False`.\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape (n_samples, n_features), calculate mean for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scale = StandardScaler().fit_transform(X[var_name])\n",
    "X = X.assign(variables={var_name + \"_scaled\":(('sampling', 'feature'), X_scale)})\n",
    "#X = X.assign(variables={var_name + \"_scaled\":(('feature', 'sampling'), X_scale)})\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scale.min())\n",
    "print(X_scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[var_name].plot.hist(bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalizer__ : shape (n_samples, n_features), calculate mean for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import Normalizer\n",
    "#X_scale = Normalizer().fit_transform(X[var_name])\n",
    "#X = X.assign(variables={var_name + \"_scaled\":(('sampling', 'feature'), X_scale)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_scale.min())\n",
    "#print(X_scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[var_name].plot.hist(bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MinMaxScaler__: scale each feature. shape (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#X_scale = MinMaxScaler().fit_transform(X[var_name])\n",
    "#X = X.assign(variables={var_name + \"_scaled\":(('sampling', 'feature'), X_scale)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_scale.min())\n",
    "#print(X_scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[var_name].min().values)\n",
    "#print(X[var_name].max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[var_name].plot.hist(bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __5) PCA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply __Principal component analysis__ (PCA):\n",
    "Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n",
    "If `0 < n_components < 1` and `svd_solver == 'full'`, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 0.99, svd_solver = 'full')\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pca.fit(X[var_name + \"_scaled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pb = plt.bar(range(pca.n_components_), pca.explained_variance_ratio_)\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('Percentage of variance explained by each of the selected components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca.transform(X[var_name + \"_scaled\"])\n",
    "np.shape(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(variables={var_name + \"_reduced\":(('sampling', 'feature_reduced'),X_reduced)})\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "k = 7 # number of classes\n",
    "model = mixture.GaussianMixture(n_components=k, covariance_type='full')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cluster import KMeans\n",
    "#k=7\n",
    "#model = KMeans(n_clusters=7)\n",
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fit model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X[var_name + \"_reduced\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predict labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labels = model.predict(X[var_name + \"_reduced\"])\n",
    "X_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_labels.min())\n",
    "print(X_labels.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(variables={\"GMM_labels\":(('sampling'),X_labels)})\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate other __stadistics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict posterior probability of each component given the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proba = model.predict_proba(X[var_name + \"_reduced\"])\n",
    "np.shape(X_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(variables={\"GMM_post\":(('sampling','k'),X_proba)})\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantiles we want to calculate\n",
    "q = [0.05, 0.5, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_quantiles = X[var_name].where(X['GMM_labels']==0, drop=True).quantile(q, dim='sampling')\n",
    "for yi in range(1,k):\n",
    "    m_quantiles = xr.concat((m_quantiles, X[var_name].where(X['GMM_labels']==yi, drop=True).quantile(q, dim='sampling')), dim='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(variables={var_name + \"_Q\":(('k','quantile','feature'), m_quantiles)})\n",
    "X = X.assign_coords(coords={'quantile': q})\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate quantiles scaled variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantiles we want to calculate\n",
    "q = [0.05, 0.5, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_quantiles = X[var_name].where(X['GMM_labels']==0, drop=True).quantile(q, dim='sampling')\n",
    "for yi in range(1,k):\n",
    "    m_quantiles = xr.concat((m_quantiles, X[var_name + '_scaled'].where(X['GMM_labels']==yi, drop=True).quantile(q, dim='sampling')), dim='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(variables={var_name + '_scaled'+ \"_Q\":(('k','quantile','feature'), m_quantiles)})\n",
    "X = X.assign_coords(coords={'quantile': q})\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpost = X[\"GMM_post\"].max(dim=\"k\")\n",
    "K = len(X[\"GMM_labels\"])\n",
    "robust = (maxpost - 1. / K) * K / (K - 1.)\n",
    "\n",
    "Plist = [0, 0.33, 0.66, 0.9, .99, 1]\n",
    "rowl0 = ('Unlikely', 'As likely as not', 'Likely', 'Very Likely', 'Virtually certain')\n",
    "robust_id = np.digitize(robust, Plist) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.assign(variables={\"GMM_robustness\":(('sampling'), robust), \"GMM_robustness_cat\":(('sampling'), robust_id)})\n",
    "X[\"GMM_robustness_cat\"].attrs['legend'] = rowl0\n",
    "print(X)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Unstack dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_labels = X.unstack('sampling')\n",
    "#ds_labels = ds_labels.sortby(['lat','lon'])\n",
    "# same lat and lon values in mask and in results\n",
    "mask = stacked_mask.unstack()\n",
    "ds_labels = ds_labels.reindex_like(mask)\n",
    "print(ds_labels)\n",
    "#ds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy atributtes\n",
    "ds_labels.attrs = ds.attrs\n",
    "ds_labels.lat.attrs = ds.lat.attrs\n",
    "ds_labels.lon.attrs = ds.lon.attrs\n",
    "#include time coord for save_BlueCloud function\n",
    "ds_labels = ds_labels.assign_coords({'time': ds.time.values})\n",
    "ds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_labels['CHL_reduced'].coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesing plots\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Plotter_OR(ds_labels, model, coords_dict={'latitude':'lat', 'longitude':'lon', 'feature': 'feature'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scatter plot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.scatter_PDF(var_name = var_name + '_reduced')\n",
    "P.save_BlueCloud('figures/scatter_PDF_EX_chl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BIC__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dist = 50 # correlation distance in km\n",
    "Nrun = 10 # number of runs for each k\n",
    "NK = 20 # max number of classes to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIC, BIC_min = BIC_calculation(X=X, coords_dict={'latitude':'lat', 'longitude':'lon'}, \n",
    "                               corr_dist=corr_dist,\n",
    "                               feature_name='feature_reduced', var_name= 'CHL_reduced',\n",
    "                               Nrun=Nrun, NK=NK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_BIC(BIC, NK=NK)\n",
    "P.save_BlueCloud('figures/BIC_EX_chl.png', bic_fig='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Plotter_OR(ds_labels, model, coords_dict={'latitude':'lat', 'longitude':'lon', 'feature': 'feature'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __1) Quantiles time series__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median and other quantiles representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.tseries_structure(q_variable = var_name + '_Q', start_month=6, ylabel='Temperature (K)')\n",
    "P.save_BlueCloud('figures/tseries_struc_EX_chl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All median time series in the same plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.tseries_structure_comp(q_variable = var_name + '_Q', plot_q= 'all', ylabel='Temperature (K)', start_month=6)\n",
    "P.save_BlueCloud('figures/tseries_struc_comp_EX_chl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If temperature is normalized (__how we can interpret this figure?__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.tseries_structure(q_variable = var_name + '_scaled' + '_Q', start_month=6, ylabel='Temperature (K)')\n",
    "P.save_BlueCloud('figures/tseries_struc_EX_norm_chl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __2) Spatial distribution of classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.spatial_distribution()\n",
    "P.save_BlueCloud('figures/spatial_distr_EX_chl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3) Robustness__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.plot_robustness()\n",
    "P.save_BlueCloud('figures/robustness_EX_chl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __4) Classes pie chart__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.pie_classes()\n",
    "P.save_BlueCloud('figures/pie_chart_EX_chl.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
